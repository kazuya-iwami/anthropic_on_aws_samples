{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude Opus 4.6 on Amazon Bedrock — 機能チュートリアル\n",
    "\n",
    "このノートブックでは、**Claude Opus 4.6** (`global.anthropic.claude-opus-4-6-v1`) で利用可能な主要機能を Amazon Bedrock 経由で実際に動かしながら学びます。\n",
    "\n",
    "## 取り扱う機能\n",
    "\n",
    "- Adaptive Thinking + Effort\n",
    "    - リクエストの複雑さに応じて Claude が自律的に思考の深さを調整する機能。effortパラメータで思考量のガイダンスを指定可能                                                                                                \n",
    "- 1M コンテキストウィンドウ (Beta)\n",
    "    - デフォルト 200K トークンのコンテキストウィンドウを 1Mトークンに拡張し、大量のドキュメントやコードベースを一度に処理可能にする                                     \n",
    "- 最大 128K の出力トークン\n",
    "    - 出力トークンの上限を最大 128K トークンまで指定可能。長文のコード生成や翻訳、レポート作成に有用                  \n",
    "- 構造化出力 - JSON Outputs\n",
    "    - LLM のレスポンスを指定した JSON Schema に厳密に準拠させ、型安全でパースエラーのない出力を保証する機能\n",
    "- Compaction (Beta)\n",
    "    - 長い会話でコンテキストウィンドウのトークン数がしきい値を超えた際に、会話を自動要約して圧縮し、会話を継続可能にする機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## セットアップ\n",
    "\n",
    "### 前提条件\n",
    "\n",
    "- Python 3.9+\n",
    "- boto3 1.42+\n",
    "- AWS 認証情報が設定済み (`AWS_PROFILE` or デフォルト認証チェーン)\n",
    "\n",
    "#### Amazon Bedrockの利用に必要なIAMの権限について\n",
    "\n",
    "本サンプルでは、以下のIAMのActionを実行する権限が必要です。初回モデルアクセス時は後述するAWS Marketplaceの権限も必要です。\n",
    "\n",
    "- bedrock:InvokeModel\n",
    "- bedrock:InvokeModelWithResponseStream\n",
    "\n",
    "####  Anthropic モデルを初めて使う場合\n",
    "\n",
    "Anthropicのモデルを初めて利用する場合、AWSアカウント、もしくはOrganizationごとに一回、ユースケース情報の提出が必要です。\n",
    "\n",
    "- Amazon Bedrockコンソールのモデルカタログページから、「Submit use case details」ボタンを押し、フォームを入力\n",
    "- 送信後、即座にアクセスが有効化されます\n",
    "\n",
    "####  AWS Marketplace 権限について\n",
    "\n",
    "Bedrockのサードパーティモデル（Anthropic含む）は、モデルごとに、初回呼び出し時に自動でアカウントに有効化されます。この自動有効化には以下のIAM権限が必要です：\n",
    "\n",
    "- aws-marketplace:Subscribe\n",
    "- aws-marketplace:ViewSubscriptions\n",
    "\n",
    "> **Note:** 一度有効化されれば、以降の呼び出しにはMarketplace権限は不要です\n",
    "\n",
    "#### EULA\n",
    "モデル初回呼び出し時に該当モデルの[エンドユーザーライセンス契約（EULA）]に同意したものとみなされます。\n",
    "\n",
    "詳細はこちらをご覧ください: [Manage access to Amazon Bedrock foundation models](https://docs.aws.amazon.com/ja_jp/bedrock/latest/userguide/model-access.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは共通で使うライブラリのインポートと Bedrock クライアントの初期化を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最新のboto3を利用\n",
    "%pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 利用するAWSのProfile名を指定してください。\n",
    "AWS_PROFILE = \"\"\n",
    "AWS_REGION = os.environ.get(\"AWS_REGION\", \"ap-northeast-1\")\n",
    "MODEL_ID = os.environ.get(\"MODEL_ID\", \"global.anthropic.claude-opus-4-6-v1\")\n",
    "\n",
    "if AWS_PROFILE:\n",
    "    session = boto3.Session(profile_name=AWS_PROFILE, region_name=AWS_REGION)\n",
    "    # print(f\"Profile:  {AWS_PROFILE}\")\n",
    "else:\n",
    "    session = boto3.Session(region_name=AWS_REGION)\n",
    "\n",
    "# 現在の AWS Identity を確認\n",
    "sts = session.client(\"sts\", region_name=AWS_REGION)\n",
    "identity = sts.get_caller_identity()\n",
    "# print(f\"Account:  {identity['Account']}\")\n",
    "# print()\n",
    "\n",
    "print(f\"リージョン: {AWS_REGION}\")\n",
    "print(f\"モデル ID:  {MODEL_ID}\")\n",
    "\n",
    "def truncate_for_display(result, max_chars=900):\n",
    "    \"\"\"invoke_model レスポンスの表示用ユーティリティ。\n",
    "    長い text/thinking ブロックを切り詰め、signature を省略してJSON全体の構造を確認しやすくする。\"\"\"\n",
    "    display = copy.deepcopy(result)\n",
    "    for block in display.get(\"content\", []):\n",
    "        if block.get(\"type\") == \"text\":\n",
    "            t = block.get(\"text\", \"\")\n",
    "            if len(t) > max_chars:\n",
    "                block[\"text\"] = t[:max_chars] + f\"... (truncated, total: {len(t)} chars)\"\n",
    "        elif block.get(\"type\") == \"thinking\":\n",
    "            t = block.get(\"thinking\", \"\")\n",
    "            if len(t) > max_chars:\n",
    "                block[\"thinking\"] = t[:max_chars] + f\"... (truncated, total: {len(t)} chars)\"\n",
    "            if \"signature\" in block:\n",
    "                block[\"signature\"] = \"(omitted)\"\n",
    "    return display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read_timeout について\n",
    "\n",
    "boto3 (botocore) の HTTP クライアントには **`read_timeout`** というパラメータがあり、デフォルトは **60 秒** です。これはサーバーからのレスポンスデータ読み取りを待つ最大時間を意味します。\n",
    "\n",
    "Bedrock で大規模モデルを使用する場合、**60 秒では足りず `ReadTimeoutError` が発生** することがあります。\n",
    "\n",
    "これを回避するには `botocore.config.Config` の `read_timeout` を明示的に延長します。\n",
    "\n",
    "> **Note:** ストリーミング API (`invoke_model_with_response_stream`) ではチャンクが逐次到着するため read_timeout の問題は起きにくいですが、非ストリーミングの `invoke_model` では出力全体の生成完了まで待つ必要があるため注意が必要です。\n",
    "\n",
    "**参考:**\n",
    "- [Amazon Bedrock の大規模モデル呼び出しにおける読み取りタイムアウトの解決方法](https://repost.aws/ja/knowledge-center/bedrock-large-model-read-timeouts)\n",
    "- [botocore Config リファレンス](https://botocore.amazonaws.com/v1/documentation/api/latest/reference/config.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "\n",
    "bedrock = session.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=AWS_REGION,\n",
    "    config=Config(read_timeout=300),  # 300秒 (5分) に延長\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## モデルへのアクセス方法 — InvokeModel API と Converse API\n",
    "\n",
    "Bedrock で Claude を呼び出すには主に 2 つの API があります。\n",
    "\n",
    "| API | 特徴 |\n",
    "|-----|------|\n",
    "| **Converse API** (`bedrock.converse`) | モデル非依存の統一インターフェース。Bedrock 上の全モデルで共通の形式を利用可能。基本的な対話・ストリーミング・ツール利用に推奨。一部のClaudeの機能の利用に制限あり |\n",
    "| **InvokeModel API** (`bedrock.invoke_model`) | 基盤モデルの推論を呼び出すためのAPI。各モデルプロバイダー固有の仕様に合わせてパラメーターを指定することが可能であり、Anthropic Messages API 形式のリクエストボディを直接送信できる |\n",
    "\n",
    "このサンプルでは、機能をシンプルに紹介するため、InvokeModel API を利用する例で説明する。Converse API での実装については末尾のAppendixに記載。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 256,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"日本の首都はどこですか？\"\n",
    "        }\n",
    "    ],\n",
    "})\n",
    "\n",
    "response = bedrock.invoke_model(\n",
    "    modelId=MODEL_ID,\n",
    "    body=body\n",
    ")\n",
    "\n",
    "result = json.loads(response[\"body\"].read())\n",
    "text = result[\"content\"][0][\"text\"]\n",
    "print(f\"質問: 日本の首都はどこですか？\")\n",
    "print(f\"回答: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### レスポンス構造の確認\n",
    "\n",
    "InvokeModel API のレスポンスにはトークン使用量などのメタデータも含まれています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メタデータの確認\n",
    "print(json.dumps(truncate_for_display(result), ensure_ascii=False, indent=2))\n",
    "print(f\"停止理由:     {result['stop_reason']}\")\n",
    "print(f\"入力トークン: {result['usage']['input_tokens']}\")\n",
    "print(f\"出力トークン: {result['usage']['output_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Adaptive Thinking + Effort\n",
    "\n",
    "**Adaptive Thinking** は Claude Opus 4.6 で推奨される Extended Thinking の利用方法です。\n",
    "\n",
    "従来の Extended Thinking では固定の thinking token budget を手動設定する必要がありましたが、Adaptive Thinking ではリクエストの複雑さに応じて Claude が自律的に **「考えるかどうか」「どの程度深く考えるか」** を判断します。\n",
    "\n",
    "### Effort パラメータ\n",
    "\n",
    "`output_config.effort` を組み合わせることで、思考量のガイダンスを与えられます。\n",
    "\n",
    "| effort level | 挙動 |\n",
    "|-------------|------|\n",
    "| `max` | 常に思考し、思考の深さに制約なし (**Opus 4.6 専用**) |\n",
    "| `high` (デフォルト) | 常に思考する。複雑なタスクで深い推論を行う |\n",
    "| `medium` | 適度に思考する。非常に簡単な質問では思考をスキップする場合がある |\n",
    "| `low` | 思考を最小限にする。速度優先の簡単なタスクで思考をスキップ |\n",
    "\n",
    "### 課金について\n",
    "\n",
    "Adaptive Thinking で使われる thinking トークンは **出力トークンとして課金** され、`max_tokens` の対象に含まれます。\n",
    "\n",
    "### 実装ポイント\n",
    "\n",
    "- `\"thinking\": {\"type\": \"adaptive\"}` と `\"output_config\": {\"effort\": \"xxx\"}` を指定\n",
    "\n",
    "**参考:**\n",
    "- [Adaptive Thinking (Anthropic)](https://platform.claude.com/docs/en/build-with-claude/adaptive-thinking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 4096,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"LLMにおいて、なぜChain of Thoughtが有効なのか、LLMの技術トレンドの中での位置付けを踏まえて考えて下さい\",\n",
    "        }\n",
    "    ],\n",
    "    \"thinking\": {\"type\": \"adaptive\"},\n",
    "    \"output_config\": {\"effort\": \"medium\"},\n",
    "})\n",
    "\n",
    "response = bedrock.invoke_model(modelId=MODEL_ID, body=body)\n",
    "result = json.loads(response[\"body\"].read())\n",
    "\n",
    "# レスポンス全体の構造を確認 (長いブロックは切り詰め表示)\n",
    "# thinking 有効時は content 配列に text / thinking ブロックが混在する\n",
    "print(json.dumps(truncate_for_display(result), ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出力 (thinking あり)\n",
    "\n",
    "Adaptive Thinking が有効で、かつ Claude が思考を行った場合、レスポンスは以下のような構造になります:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"content\": [\n",
    "    {\n",
    "      \"type\": \"text\",\n",
    "      \"text\": \"\\n\\n\"                    // 空の text (thinking の前に挿入される場合がある)\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"thinking\",\n",
    "      \"thinking\": \"...\",               // 内部思考\n",
    "      \"signature\": \"...\"              // 改ざん防止用署名 (マルチターン時にそのまま返す)\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"text\",\n",
    "      \"text\": \"回答...\"             // ユーザーに返す回答\n",
    "    }\n",
    "  ],\n",
    "  \"stop_reason\": \"end_turn\",\n",
    "  \"usage\": {\n",
    "    \"input_tokens\": ...,\n",
    "    \"output_tokens\": ...               // thinking トークンも output_tokens に含まれる\n",
    "  }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effort レベルの比較\n",
    "\n",
    "同じ質問を `low` effort で投げてみて、思考量の違いを確認します。  \n",
    "簡単な質問を `low` で投げると thinking ブロックが生成されない場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_low = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 4096,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"日本の首都はどこですか？\",\n",
    "        }\n",
    "    ],\n",
    "    \"thinking\": {\"type\": \"adaptive\"},\n",
    "    \"output_config\": {\"effort\": \"low\"},\n",
    "})\n",
    "\n",
    "response_low = bedrock.invoke_model(modelId=MODEL_ID, body=body_low)\n",
    "result_low = json.loads(response_low[\"body\"].read())\n",
    "\n",
    "print(json.dumps(truncate_for_display(result_low), ensure_ascii=False, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出力 (thinking なし)\n",
    "\n",
    "`effort=low` かつ簡単な質問の場合、Claude は思考をスキップします。レスポンスは以下のようにシンプルになります:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"content\": [\n",
    "    {\n",
    "      \"type\": \"text\",   // thinking ブロックなし、text のみ\n",
    "      \"text\": \"回答...\"\n",
    "    }   \n",
    "  ],\n",
    "  \"stop_reason\": \"end_turn\",\n",
    "  \"usage\": {\"input_tokens\": ..., \"output_tokens\": ...}\n",
    "}\n",
    "```\n",
    "\n",
    "thinking ブロックが生成されない分 `output_tokens` が大幅に少なく、レイテンシ・コストともに低くなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 1M コンテキストウィンドウ\n",
    "\n",
    "Claude Opus 4.6 のデフォルトは **200K (20万) トークン** コンテキストウィンドウですが、beta flag を指定することで **1M (100万) トークン** のコンテキストウィンドウを利用できます。\n",
    "\n",
    "1M トークンというのは、ハリー・ポッターと賢者の石 の本 3冊弱 にも相当する量であり、大量のドキュメント分析、長大なコードベースの理解、膨大なログの検索などに有用です。\n",
    "\n",
    "### 実装ポイント\n",
    "\n",
    "- `\"anthropic_beta\": [\"context-1m-2025-08-07\"] `を指定\n",
    "- モデル ID は 200K トークン と 1M トークン モデルで同一。\n",
    "\n",
    "### 注意事項\n",
    "\n",
    "- 1M トークンの処理には今回のような処理でも 数10秒掛かることがあります。\n",
    "- 入力トークン課金が大きくなるため、prompt caching との併用を検討すること。\n",
    "- 200K トークン モデルに対して、それ以上のトークン数のpromptを入力した場合、 `ValidationException: prompt is too long: 200001 tokens > 200000 maximum` のようなエラーとなる。\n",
    "\n",
    "**参考:**\n",
    "- [Context Windows - 1M token context window (Anthropic)](https://platform.claude.com/docs/en/build-with-claude/context-windows#1-m-token-context-window)\n",
    "- [Context Window と Extended Thinking (Anthropic)](https://platform.claude.com/docs/en/build-with-claude/context-windows#the-context-window-with-extended-thinking)\n",
    "- [Bedrock Count Tokens](https://docs.aws.amazon.com/bedrock/latest/userguide/count-tokens.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needle-in-Haystack テスト\n",
    "\n",
    "1M トークン モデルの挙動確認のため、大量のダミーテキストの中にパスフレーズを埋め込み、モデルに見つけさせるテストを行います。\n",
    "\n",
    "#### Count Tokens による事前見積もり\n",
    "\n",
    "大量のテキストを送信する前に `bedrock.count_tokens()` で入力トークン数を事前に確認できます。API 呼び出しなしでトークン数が分かるため、コスト見積もりやコンテキストウィンドウ超過の防止に有用です。\n",
    "\n",
    "- `count_tokens` は **ベースモデル ID** (`anthropic.claude-xxxx`) のみ対応。Cross-region inference profile (`global.anthropic.claude-xxx`) 等は指定不可\n",
    "- 結果は実際の `usage.input_tokens` より約 20 トークン多い (内部システムトークンを含む。課金対象外)\n",
    "- `count_tokens` 自体は無料 (レート制限あり)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索対象のパスフレーズ (needle)\n",
    "NEEDLE = \"OPUS46-BEDROCK\"\n",
    "\n",
    "def build_long_text(lines_needed: int = 5000) -> str:\n",
    "    \"\"\"指定行数のダミーテキストを生成し、中間に needle を埋め込む\"\"\"\n",
    "    line = \"Amazon Web Services は、コンピューティング、ストレージ、データベース、分析、ネットワーキング、モバイル、デベロッパーツール、管理ツール、IoT、セキュリティ、エンタープライズアプリケーションなど、グローバルなクラウドベースの製品を幅広く提供しています。これらはオンデマンドで数秒で利用でき、従量制料金が適用されます。データウェアハウスからデプロイツール、ディレクトリ、コンテンツ配信まで、200 を超える AWS サービスが利用可能です。\\n\"\n",
    "    mid = lines_needed // 2\n",
    "\n",
    "    lines = []\n",
    "    for i in range(lines_needed):\n",
    "        if i == mid:\n",
    "            lines.append(f\"パスフレーズは {NEEDLE} です。\\n\")\n",
    "        lines.append(line)\n",
    "\n",
    "    return \"\".join(lines)\n",
    "\n",
    "text = build_long_text()\n",
    "prompt = f\"{text}\\n\\n上記テキストに含まれる「秘密のパスフレーズ」を正確に抜き出してください。\"\n",
    "\n",
    "# 1M context window モデルを利用する際は anthropic_beta: context-1m-2025-08-07 flag が必須\n",
    "body = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"anthropic_beta\": [\"context-1m-2025-08-07\"],\n",
    "    \"max_tokens\": 256,\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "})\n",
    "\n",
    "# count_tokens で事前にトークン数を確認\n",
    "# ベースモデルID のみ対応 (global.から始まるグローバルクロスリージョン推論プロファイルの利用は不可)\n",
    "\n",
    "COUNT_TOKENS_MODEL_ID = MODEL_ID.replace(\"global.\", \"\")\n",
    "response = bedrock.count_tokens(\n",
    "    modelId=COUNT_TOKENS_MODEL_ID,\n",
    "    input={\"invokeModel\": {\"body\": body}},\n",
    ")\n",
    "print(f\"推定入力トークン: {response['inputTokens']:,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock.invoke_model(modelId=MODEL_ID, body=body)\n",
    "result = json.loads(response[\"body\"].read())\n",
    "\n",
    "answer = result[\"content\"][0][\"text\"]\n",
    "print(f\"回答: {answer}\")\n",
    "\n",
    "usage = result[\"usage\"]\n",
    "print(f\"入力トークン: {usage['input_tokens']:,}\")\n",
    "print(f\"出力トークン: {usage['output_tokens']:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. 最大 128K の出力トークン\n",
    "\n",
    "Claude Opus 4.6 では `max_tokens` を最大 **128,000 トークン** まで指定できます。  \n",
    "長文のコード生成、翻訳、レポート作成などに有用です。\n",
    "\n",
    "### 実装ポイント\n",
    "\n",
    "- Opus 4.6 では出力トークンの上限が 64K トークンから 128K トークンに拡大されました。\n",
    "- `invoke_model` / `invoke_model_with_response_stream` の body に `\"max_tokens\": 128000` を指定\n",
    "- Converse API でも `inferenceConfig.maxTokens` で指定可能\n",
    "- Streamingを利用しない場合は、Bedrock clientを作成する際の`read_timeout`の値を十分に大きな値 (数10分以上) に設定してください。\n",
    "\n",
    "### Streamingを利用した 128K の出力トークン の機能検証\n",
    "\n",
    "InvokeModelWithResponseStream APIを利用して、ストリームで約100Kトークンのテキストを出力させます。\n",
    "\n",
    "100K トークンの出力には、Claude Opus 4.6で数10分掛かるのでご留意下さい。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力ファイルパス\n",
    "OUTPUT_FILE = \"output/output_128k_stream.txt\"\n",
    "\n",
    "body = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 128000,\n",
    "    \"thinking\": {\"type\": \"adaptive\"},\n",
    "    \"output_config\": {\"effort\": \"max\"},\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"あなたは長編SF作家です。10万文字の長編小説を必ず一回の出力で執筆してください。\n",
    "\n",
    "## フェーズ1: 構想\n",
    "最初に以下を検討してください：\n",
    "- テーマ・世界観・時代設定\n",
    "- 主要キャラクター（5〜8名）\n",
    "- プロットの骨子\n",
    "\n",
    "## フェーズ2: 章立て\n",
    "章立てを設計：\n",
    "- 大章10個（タイトル・役割・目標）\n",
    "- 各大章にサブ章8〜10個（シーン概要）\n",
    "\n",
    "## フェーズ3: 執筆ルール\n",
    "1. 各サブ章は約1000文字\n",
    "2. 大章開始時に全体章立てを再掲：\n",
    "   【全体章立て】第1章←現在 / 第2章 / ... / 第10章\n",
    "3. 形式：【第X章 - X-Y】タイトル → 本文\n",
    "4. 前後の連続性・伏線を意識\n",
    "\"\"\",\n",
    "        }\n",
    "    ],\n",
    "})\n",
    "\n",
    "# ストリーミングで呼び出し\n",
    "response = bedrock.invoke_model_with_response_stream(modelId=MODEL_ID, body=body)\n",
    "\n",
    "current_block_type = None\n",
    "\n",
    "# バッファリングを無効化 (buffering=1: 行バッファ) してリアルタイムにファイルへ反映\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\", buffering=1) as f:\n",
    "    for event in response[\"body\"]:\n",
    "        chunk = json.loads(event[\"chunk\"][\"bytes\"])\n",
    "        event_type = chunk[\"type\"]\n",
    "\n",
    "        if event_type == \"content_block_start\":\n",
    "            current_block_type = chunk.get(\"content_block\", {}).get(\"type\")\n",
    "            if current_block_type == \"thinking\":\n",
    "                f.write(\"=== Thinking Block ===\\n\")\n",
    "                print(\"=== Thinking Block ===\")\n",
    "\n",
    "        elif event_type == \"content_block_stop\":\n",
    "            if current_block_type == \"thinking\":\n",
    "                f.write(\"\\n=== End Thinking Block ===\\n\\n\")\n",
    "                print(\"\\n=== End Thinking Block ===\\n\")\n",
    "            current_block_type = None\n",
    "\n",
    "        elif event_type == \"content_block_delta\":\n",
    "            delta = chunk[\"delta\"]\n",
    "            if delta[\"type\"] == \"text_delta\":\n",
    "                text = delta[\"text\"]\n",
    "                print(text, end=\"\", flush=True)\n",
    "                f.write(text)\n",
    "            elif delta[\"type\"] == \"thinking_delta\":\n",
    "                thinking = delta[\"thinking\"]\n",
    "                print(thinking, end=\"\", flush=True)\n",
    "                f.write(thinking)\n",
    "\n",
    "        elif event_type == \"message_delta\":\n",
    "            stop_reason = chunk[\"delta\"].get(\"stop_reason\", \"\")\n",
    "            output_tokens = chunk[\"usage\"][\"output_tokens\"]\n",
    "            print(f\"\\n\\n--- ストリーム完了 ---\")\n",
    "            print(f\"停止理由:     {stop_reason}\")\n",
    "            print(f\"出力トークン: {output_tokens:,}\")\n",
    "            print(f\"出力ファイル: {OUTPUT_FILE}\")\n",
    "\n",
    "        elif event_type == \"message_start\":\n",
    "            input_tokens = chunk[\"message\"][\"usage\"][\"input_tokens\"]\n",
    "            print(f\"[入力トークン: {input_tokens:,}]\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. 構造化出力 (JSON Outputs)\n",
    "\n",
    "### Structured Outputs とは\n",
    "\n",
    "**Structured Outputs** は、LLMのレスポンスを **指定したスキーマに厳密に準拠した形式に制約する** 機能です。\n",
    "\n",
    "通常の LLM 出力はあくまでも自由形式のテキストなので、プログラムで後処理する際に以下のような問題が起こり得ます:\n",
    "\n",
    "- JSON のパースエラー（不正な構文、閉じ括弧の欠落など）\n",
    "- 必須フィールドの欠落\n",
    "- 型の不一致（数値が期待される場所に文字列が返る等）\n",
    "- スキーマ違反に対するリトライ処理の必要性\n",
    "\n",
    "Structured Outputs は **Constrained Decoding（制約付きデコーディング）** により、上記を解決する機能です。\n",
    "\n",
    "### 2 つのアプローチ\n",
    "\n",
    "Structured Outputs には **2 つの補完的な機能** があります:\n",
    "\n",
    "| 機能 | パラメータ | 制約対象 | 用途 |\n",
    "|------|-----------|---------|------|\n",
    "| **JSON outputs** | `output_config.format` | Claude の **レスポンス形式** (Claude が何を返すか) | データ抽出、レポート生成、API レスポンス整形 |\n",
    "| **Strict tool use** | `strict: true` (Tool 定義内) | Tool の **入力パラメータ** (Claude がどう関数を呼ぶか) | エージェントワークフロー、型安全な関数呼び出し |\n",
    "\n",
    "これらは独立して使うことも、**同じリクエスト内で併用** することもできます。\n",
    "\n",
    "ここのサンプルでは、 **JSON outputs** を利用します。\n",
    "\n",
    "JSON Outputs はデータ抽出や、分類・タグ付け、後段のAPIとの連携等「**Claude の出力をそのまま後続プログラムに渡したい**」場面で有効です:\n",
    "\n",
    "#### 実装ポイント\n",
    "\n",
    "- JSON outputsを利用する場合は、`output_config.format` にスキーマを指定します。\n",
    "- **初回リクエストの遅延**: スキーマごとに Grammar コンパイルが走るため初回はやや遅い。コンパイル結果は **24 時間キャッシュ** され、以降は高速\n",
    "- Adaptive Thinking と併用可能。ただしレスポンスに `text(\"\\n\\n\")` → `thinking` → `text(JSON)` の 3 ブロックが含まれるため、**最後の text ブロック** から JSON をパースする必要がある\n",
    "\n",
    "**参考:**\n",
    "- [Structured Outputs (Anthropic)](https://platform.claude.com/docs/en/build-with-claude/structured-outputs)\n",
    "- [Bedrock Structured Output](https://docs.aws.amazon.com/bedrock/latest/userguide/structured-output.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力スキーマ定義\n",
    "BOOK_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"title\": {\"type\": \"string\"},\n",
    "        \"author\": {\"type\": \"string\"},\n",
    "        \"year\": {\"type\": \"integer\"},\n",
    "        \"genre\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"fiction\", \"non-fiction\", \"science\", \"history\", \"philosophy\"],\n",
    "        },\n",
    "        \"summary\": {\"type\": \"string\"},\n",
    "        \"recommended\": {\"type\": \"boolean\"},\n",
    "    },\n",
    "    \"required\": [\"title\", \"author\", \"year\", \"genre\", \"summary\", \"recommended\"],\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "print(\"スキーマ定義:\")\n",
    "print(json.dumps(BOOK_SCHEMA, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_config.format を使用して構造化出力を取得\n",
    "body = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"「2001年宇宙の旅」という本について教えてください。\",\n",
    "        }\n",
    "    ],\n",
    "    \"output_config\": {\n",
    "        \"format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"schema\": BOOK_SCHEMA,\n",
    "        }\n",
    "    },\n",
    "})\n",
    "\n",
    "response = bedrock.invoke_model(modelId=MODEL_ID, body=body)\n",
    "result = json.loads(response[\"body\"].read())\n",
    "\n",
    "# 最後の text ブロックから JSON をパース\n",
    "raw_text = result[\"content\"][-1][\"text\"]\n",
    "parsed = json.loads(raw_text)\n",
    "\n",
    "print(\"構造化出力 (JSON):\")\n",
    "print(json.dumps(parsed, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Compaction (会話圧縮)\n",
    "\n",
    "**Compaction** は、長い会話セッションでコンテキストウィンドウを効率的に管理するための機能です。\n",
    "\n",
    "会話が設定したトークンしきい値に近づくと、Claude が自動的に会話の要約を生成し、圧縮されたコンテキストで会話を継続できます。長時間のエージェンティックタスクや対話型アプリケーションに有用です。\n",
    "\n",
    "### 動作フロー\n",
    "\n",
    "1. 入力トークン数がトリガーしきい値を超えたことを検知\n",
    "2. 現在の会話の要約を生成\n",
    "3. 要約を含む compaction ブロックを作成\n",
    "4. 圧縮されたコンテキストを使って応答を継続、もしくは、一時停止してクライアントに処理を委譲\n",
    "\n",
    "### 実装ポイント\n",
    "\n",
    "- beta flag: **`compact-2026-01-12`** (必須)\n",
    "- `context_management.edits` 配列内に compaction 設定を記述\n",
    "- `pause_after_compaction=true` の場合、compaction 発生後に `stop_reason=\"compaction\"` が返る。クライアント側で compaction ブロックを取り出し、直近のメッセージと合わせて再リクエストする\n",
    "\n",
    "#### 詳細なパラメータ\n",
    "\n",
    "| パラメータ | 説明 |\n",
    "|-----------|------|\n",
    "| `type` | `\"compact_20260112\"` を指定 |\n",
    "| `trigger` | compaction を発動するタイミング。デフォルトは 150,000 トークン。最低 50,000 以上 |\n",
    "| `pause_after_compaction` | `true` にすると compaction 後に `stop_reason=\"compaction\"` で一時停止。クライアント側でハンドリング可能 |\n",
    "| `instructions` | カスタム要約プロンプト (以下のデフォルトプロンプトを完全に置き換えます) |\n",
    "\n",
    "\n",
    "デフォルトプロンプト:\n",
    "\n",
    "```You have written a partial transcript for the initial task above. Please write a summary of the transcript. The purpose of this summary is to provide continuity so you can continue to make progress towards solving the task in a future context, where the raw history above may not be accessible and will be replaced with this summary. Write down anything that would be helpful, including the state, next steps, learnings etc. You must wrap your summary in a <summary></summary> block.```\n",
    "\n",
    "**参考:**\n",
    "- [Compaction (Anthropic)](https://platform.claude.com/docs/en/build-with-claude/compaction)\n",
    "- [Bedrock Claude Compaction](https://docs.aws.amazon.com/bedrock/latest/userguide/claude-messages-compaction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "\n",
    "# Compaction デモ用に read_timeout を延長したクライアントを作成\n",
    "bedrock_compaction = session.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=AWS_REGION,\n",
    "    config=Config(read_timeout=1000),\n",
    ")\n",
    "\n",
    "# 会話履歴\n",
    "compaction_messages = []\n",
    "\n",
    "def _invoke_compaction(request_messages, pause_after_compaction=True):\n",
    "    \"\"\"Compaction 対応の invoke_model 呼び出し\"\"\"\n",
    "    body = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"anthropic_beta\": [\"compact-2026-01-12\"],\n",
    "        \"max_tokens\": 15000,\n",
    "        \"messages\": request_messages,\n",
    "        \"context_management\": {\n",
    "            \"edits\": [\n",
    "                {\n",
    "                    \"type\": \"compact_20260112\",\n",
    "                    \"trigger\": {\"type\": \"input_tokens\", \"value\": 50000},\n",
    "                    \"pause_after_compaction\": pause_after_compaction,\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"thinking\": {\"type\": \"adaptive\"},\n",
    "        \"output_config\": {\"effort\": \"high\"},\n",
    "    })\n",
    "\n",
    "    response = bedrock_compaction.invoke_model(modelId=MODEL_ID, body=body)\n",
    "    result = json.loads(response[\"body\"].read())\n",
    "\n",
    "    usage = result.get(\"usage\", {})\n",
    "    print(f\"[tokens] input: {usage.get('input_tokens', '?'):,}, output: {usage.get('output_tokens', '?'):,}\")\n",
    "    return result\n",
    "\n",
    "def chat_with_compaction(user_message):\n",
    "    \"\"\"ユーザーメッセージを送信し、compaction 発生時はハンドリングして応答を返す\"\"\"\n",
    "    compaction_messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    result = _invoke_compaction(compaction_messages, pause_after_compaction=True)\n",
    "\n",
    "    # compaction が発生して一時停止した場合のハンドリング\n",
    "    if result.get(\"stop_reason\") == \"compaction\":\n",
    "        compaction_block = result[\"content\"][0]\n",
    "\n",
    "        print(\"  [COMPACTION] Compaction が発生しました！\")\n",
    "        print(\"  レスポンス構造:\")\n",
    "        print(json.dumps(truncate_for_display(result), ensure_ascii=False, indent=2))\n",
    "\n",
    "        # 直近の 2 メッセージを保持して再構築\n",
    "        preserved = compaction_messages[-2:] if len(compaction_messages) >= 2 else compaction_messages[:]\n",
    "        messages_after = [\n",
    "            {\"role\": \"assistant\", \"content\": [compaction_block]}\n",
    "        ] + preserved\n",
    "\n",
    "        # 圧縮済みコンテキストで再度呼び出し\n",
    "        result = _invoke_compaction(messages_after, pause_after_compaction=False)\n",
    "\n",
    "        compaction_messages.clear()\n",
    "        compaction_messages.extend(messages_after)\n",
    "\n",
    "    # 最終応答を履歴に追加\n",
    "    compaction_messages.append({\"role\": \"assistant\", \"content\": result[\"content\"]})\n",
    "\n",
    "    text_parts = [b[\"text\"] for b in result[\"content\"] if b[\"type\"] == \"text\"]\n",
    "    return \"\\n\".join(text_parts).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### マルチターン会話で Compaction を体験\n",
    "\n",
    "Web アプリケーションの実装について 9 往復の対話を行います。会話が長くなると入力トークンがしきい値 (50,000) を超え、自動的に Compaction が発動します。\n",
    "\n",
    "`[COMPACTION]` のログが表示されたら、会話が圧縮されて継続されていることを確認してください。対話が終わるまで20分程掛かる場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"PythonでユーザーのCRUD操作ができるREST APIを作る際の実装について教えて\",\n",
    "    \"データベースに接続してデータを永続化できるように修正して\",\n",
    "    \"認証機能を追加して。ログインとトークン検証ができるようにして\",\n",
    "    \"このAPIを使うフロントエンドを作って。ログイン画面とユーザー一覧画面を実装して\",\n",
    "    \"フロントエンドからAPIを呼び出せるように書き足して\",\n",
    "    \"バックエンドをDockerコンテナ化して\",\n",
    "    \"docker-composeでAPI、フロントエンド、DBをまとめて起動できるようにして\",\n",
    "    \"CDKでAWSにこのアプリをホストするインフラを構築するコードを書いて\",\n",
    "    \"GitHub Actionsでmainへのpush時に自動デプロイするワークフローを書いて\",\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[{i}/10] User: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    response_text = chat_with_compaction(prompt)\n",
    "    # 応答の先頭 500 文字をプレビュー表示\n",
    "    preview = response_text[:500] + \"...\" if len(response_text) > 500 else response_text\n",
    "    print(f\"\\nAssistant: {preview}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行時の挙動\n",
    "\n",
    "この例では、後半の質問で入力トークンが `trigger` の 50,000 を超え、**Compaction が自動発動** することが期待されます。\n",
    "このサンプルでは以下のようなイメージの挙動を取ります。\n",
    "\n",
    "```\n",
    "[1/9]  input:  1,000\n",
    "[2/9]  input:  8,000\n",
    "[3/9]  input: 16,000\n",
    "[4/9]  input: 25,000\n",
    "[5/9]  input: 34,000\n",
    "[6/9]  input: 43,000\n",
    "[7/9]  input: 52,000  →  50,000 超過し Compaction が発動\n",
    "        ├─ stop_reason=\"compaction\" で一時停止\n",
    "        ├─ クライアントが compaction ブロック + 直近メッセージをセットして再リクエスト\n",
    "        └─ input: 5000 に大幅に入力が削減されて再開\n",
    "[8/9]  input: 13,000\n",
    "  ...\n",
    "```\n",
    "\n",
    "#### Compaction 発動時のレスポンス構造\n",
    "\n",
    "`stop_reason` が通常の `\"end_turn\"` ではなく **`\"compaction\"`** になり、`content[0]` に要約結果が格納されます。input_tokensとoutput_tokensはそれぞれCompaction実行時に入力したトークン数と、要約を出力した際のトークン数を指します:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"content\": [\n",
    "    {\n",
    "      \"type\": \"compaction\",\n",
    "      \"content\": \"これまでの会話の要約: ユーザーはWebアプリケーション開発の...\"\n",
    "    }\n",
    "  ],\n",
    "  \"stop_reason\": \"compaction\",\n",
    "  \"usage\": {\n",
    "    \"input_tokens\": 53000, \n",
    "    \"output_tokens\": 2000\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### クライアント側のハンドリング\n",
    "\n",
    "Compaction レスポンスを受け取ったら、クライアントは **compaction ブロック + 直近のメッセージ** で messages を再構築して API を再呼び出しします。このパターンは [公式ドキュメント](https://platform.claude.com/docs/en/build-with-claude/compaction) でも紹介されている方法です。\n",
    "\n",
    "このサンプルコードでは `compaction_messages[-2:]` で直近 **2 メッセージ** を保持しています。例えば 7 回目の質問で発動した場合:\n",
    "\n",
    "```python\n",
    "# compaction_messages = [user1, asst1, ..., user6, asst6, user7]\n",
    "# compaction_messages[-2:] → [asst6, user7]\n",
    "\n",
    "messages_after = [\n",
    "    {\"role\": \"assistant\", \"content\": [compaction_block]},    # 以前の会話の要約\n",
    "    {\"role\": \"assistant\", \"content\": [...]},                 # 6回目のClaudeの回答 (asst6)\n",
    "    {\"role\": \"user\",      \"content\": \"docker-composeで...\"}, # 7回目のユーザーからの質問 (user7)\n",
    "]\n",
    "```\n",
    "\n",
    "5回目以前のやりとりは compaction ブロックの要約にのみ含まれ、原文は破棄されます。API は compaction ブロックより前のメッセージを自動的に無視するため、要約 + 直近の原文で会話の連続性を保ちつつ、入力トークンを大幅に削減できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## まとめ\n",
    "\n",
    "このノートブックで扱った機能を振り返ります。\n",
    "\n",
    "- Adaptive Thinking + Effort\n",
    "    - リクエストの複雑さに応じて Claude が自律的に思考の深さを調整する機能。effortパラメータで思考量のガイダンスを指定可能                                                                                                \n",
    "- 1M コンテキストウィンドウ (Beta)\n",
    "    - デフォルト 200K トークンのコンテキストウィンドウを 1Mトークンに拡張し、大量のドキュメントやコードベースを一度に処理可能にする                                     \n",
    "- 最大 128K の出力トークン\n",
    "    - 出力トークンの上限を最大 128K トークンまで指定可能。長文のコード生成や翻訳、レポート作成に有用                  \n",
    "- 構造化出力 - JSON Outputs\n",
    "    - LLM のレスポンスを指定した JSON Schema に厳密に準拠させ、型安全でパースエラーのない出力を保証する機能\n",
    "- Compaction (Beta)\n",
    "    - 長い会話でコンテキストウィンドウのトークン数がしきい値を超えた際に、会話を自動要約して圧縮し、会話を継続可能にする機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Appendix - Converse API で実装するパターンの設定方法\n",
    "\n",
    "InvokeModel API を利用する場合、Amazon Bedrock で利用可能な全ての Claude の機能が利用できる。Converse APIでも主要な機能は対応しているが、2026年2月時点で  Converse API は以下の機能が未対応となっている。\n",
    "\n",
    "- Compaction: ValidationException: The compact beta feature is not currently supported on the Converse and ConverseStream APIs となり失敗\n",
    "- Tool Search Tool: The tool-search-tool beta feature is not currently supported on the Converse and ConverseStream APIs となり失敗\n",
    "- Document Citations: PDFのみ対応、txtは現状失敗する\n",
    "\n",
    "```python\n",
    "bedrock.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    messages=[...], # ここに各種パラメータを指定\n",
    "    inferenceConfig={\"maxTokens\": ...},\n",
    "    additionalModelRequestFields={...},  # ここに各種パラメータを指定\n",
    ")\n",
    "```\n",
    "\n",
    "#### Adaptive Thinking + Effort\n",
    "```python\n",
    "additionalModelRequestFields={\n",
    "    \"thinking\": {\"type\": \"adaptive\"},\n",
    "    \"output_config\": {\"effort\": \"high\"},\n",
    "}\n",
    "```\n",
    "\n",
    "#### Structured Outputs\n",
    "```python\n",
    "additionalModelRequestFields={\n",
    "    \"output_config\": {\"format\": {\"type\": \"json_schema\", \"schema\": {...}}},\n",
    "}\n",
    "```\n",
    "\n",
    "#### 1M コンテキスト\n",
    "```python\n",
    "additionalModelRequestFields={\n",
    "    \"anthropic_beta\": [\"context-1m-2025-08-07\"],\n",
    "}\n",
    "```\n",
    "\n",
    "#### Context Editing\n",
    "```python\n",
    "additionalModelRequestFields={\n",
    "    \"anthropic_beta\": [\"context-management-2025-06-27\"],\n",
    "    \"context_management\": {\n",
    "        \"edits\": [{\n",
    "            \"type\": \"clear_tool_uses_20250919\",\n",
    "            \"trigger\": {\"type\": \"input_tokens\", \"value\": 5000},\n",
    "            \"keep\": {\"type\": \"tool_uses\", \"value\": 2},  # invoke_model では整数、Converseではdict\n",
    "        }]\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "#### Computer Use / Memory Tool / Bash Tool (schema-less)\n",
    "```python\n",
    "additionalModelRequestFields={\n",
    "    \"anthropic_beta\": [\"computer-use-2025-11-24\"],  # Computer Useの場合\n",
    "    \"tools\": [{\"type\": \"computer_20251124\", \"name\": \"computer\", ...}],\n",
    "}\n",
    "```\n",
    "\n",
    "#### Fine-grained Tool Streaming — toolConfig は使わず additionalModelRequestFields のみ\n",
    "```python\n",
    "additionalModelRequestFields={\n",
    "    \"tools\": [{\"name\": \"...\", \"input_schema\": {...}, \"eager_input_streaming\": True}],\n",
    "    \"tool_choice\": {\"type\": \"any\"},\n",
    "}\n",
    "```\n",
    "\n",
    "#### Prompt Caching — cachePoint (invoke_model の cache_control とは別形式)\n",
    "```python\n",
    "messages=[\n",
    "    {\"text\": \"...\"},\n",
    "    {\"cachePoint\": {\"type\": \"default\"}},          # 5分TTL\n",
    "    # {\"cachePoint\": {\"type\": \"default\", \"ttl\": \"1h\"}},  # 1時間TTL\n",
    "]\n",
    "```\n",
    "\n",
    "### Citations (searchResult) \n",
    "```python\n",
    "messages=[{\"role\": \"user\", \"content\": [\n",
    "    {\"searchResult\": {\"source\": \"URL\", \"title\": \"...\", \"content\": [...], \"citations\": {\"enabled\": True}}},\n",
    "    {\"text\": \"質問\"},\n",
    "]}]\n",
    "```\n",
    "\n",
    "#### Citations (PDF document)\n",
    "```python\n",
    "{\"document\": {\"name\": \"doc\", \"format\": \"pdf\", \"source\": {\"bytes\": ...}, \"citations\": {\"enabled\": True}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
